{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://github.com/jocicmarko/ultrasound-nerve-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import h5py\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change the resolution to 128 * 128\n",
    "- result : 0.57521, will not use it right now\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "img_rows = 128\n",
    "img_cols = 160\n",
    "image_rows = 420\n",
    "image_cols = 580\n",
    "smooth = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation\n",
    "### code from https://www.kaggle.com/bguberfain/ultrasound-nerve-segmentation/elastic-transform-for-data-augmentation/comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate elastic image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to distort image\n",
    "def elastic_transform(image, alpha, sigma, alpha_affine, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications).\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "         Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "         Proc. of the International Conference on Document Analysis and\n",
    "         Recognition, 2003.\n",
    "\n",
    "     Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    \n",
    "    # Random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_scource = 'origin_data/'\n",
    "image_rows = 420\n",
    "image_cols = 580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## need to resize image first\n",
    "def create_elastic_train_data(img_rows, img_cols):\n",
    "    train_data_path = os.path.join(data_scource, 'train/')\n",
    "    images = os.listdir(train_data_path)\n",
    "    remove_id = np.load('img_remove_id.npy')\n",
    "\n",
    "    total = len(images) / 2 - len(set(remove_id))\n",
    "\n",
    "    imgs = np.ndarray((total, 1, img_rows, img_cols), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, 1, img_rows, img_cols), dtype=np.uint8)\n",
    "    \n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        if 'mask' in image_name:\n",
    "            continue\n",
    "        if image_name in remove_id:\n",
    "            continue\n",
    "        image_mask_name = image_name.split('.')[0] + '_mask.tif'\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        img =  cv2.resize(img, (img_cols, img_rows))\n",
    "        img_mask = cv2.resize(img_mask, (img_cols, img_rows))\n",
    "\n",
    "        \n",
    "        im_merge = np.concatenate((img[...,None], img_mask[...,None]), axis=2)\n",
    "        \n",
    "        im_merge_t = elastic_transform(im_merge, \n",
    "                                       im_merge.shape[1] * 2, im_merge.shape[1] * 0.08, im_merge.shape[1] * 0.08)\n",
    "\n",
    "        # Split image and mask\n",
    "        img = im_merge_t[...,0]\n",
    "        img_mask = im_merge_t[...,1]\n",
    "        \n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_train_t_conflict.npy', imgs)\n",
    "    np.save('imgs_mask_train_t_conflict.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/5421 images\n",
      "Done: 1000/5421 images\n",
      "Done: 2000/5421 images\n",
      "Done: 3000/5421 images\n",
      "Done: 4000/5421 images\n",
      "Done: 5000/5421 images\n",
      "Loading done.\n",
      "Saving to .npy files done.\n"
     ]
    }
   ],
   "source": [
    "create_elastic_train_data(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finish generate data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_elastic_data():\n",
    "    imgs_train_t = np.load('imgs_train_t_conflict.npy')\n",
    "    imgs_mask_train_t = np.load('imgs_mask_train_t_conflict.npy')\n",
    "    return imgs_train_t, imgs_mask_train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train_conflict.npy')\n",
    "    imgs_mask_train = np.load('imgs_mask_train_conflict.npy')\n",
    "    return imgs_train, imgs_mask_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    return imgs_test, imgs_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size = (2, 2))(conv5)\n",
    "    \n",
    "    conv5_5 = Convolution2D(1024, 3, 3, activation = 'relu', border_mode = 'same')(pool5)\n",
    "    conv5_5 = Convolution2D(512, 3, 3, activation = 'relu', border_mode = 'same')(conv5_5)\n",
    "    \n",
    "    up5_5 = merge([UpSampling2D(size = (2, 2))(conv5_5), conv5],mode = 'concat', concat_axis =1)\n",
    "    conv6_5 = Convolution2D(512, 3, 3, activation = 'relu', border_mode = 'same')(up5_5)\n",
    "    conv6_5 = Convolution2D(256, 3, 3, activation = 'relu', border_mode = 'same')(conv6_5)\n",
    "    \n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv6_5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], imgs.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i, 0] = cv2.resize(imgs[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "    return imgs_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_predict():\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-'*30)\n",
    "    imgs_train, imgs_mask_train = load_train_data()\n",
    "    imgs_train_t, imgs_mask_train_t = load_train_elastic_data()\n",
    "    \n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    "    \n",
    "    imgs_train_t = preprocess(imgs_train_t)\n",
    "    imgs_mask_train_t = preprocess(imgs_mask_train_t)\n",
    "    \n",
    "    imgs_train = np.concatenate((imgs_train, imgs_train_t), axis = 0)\n",
    "    imgs_mask_train = np.concatenate((imgs_mask_train, imgs_mask_train_t), axis = 0)\n",
    "\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet()\n",
    "    model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=16, nb_epoch=20, verbose=1, shuffle=True,\n",
    "              callbacks=[model_checkpoint])\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing test data...')\n",
    "    print('-'*30)\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = preprocess(imgs_test)\n",
    "\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('unet.hdf5')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "    np.save('imgs_mask_test.npy', imgs_mask_test)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('finish...')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep(img):\n",
    "    img = img.astype('float32')\n",
    "    img = cv2.threshold(img, 0.5, 1., cv2.THRESH_BINARY)[1].astype(np.uint8)\n",
    "    img = cv2.resize(img, (image_cols, image_rows))\n",
    "    return img\n",
    "\n",
    "\n",
    "def run_length_enc(label):\n",
    "    from itertools import chain\n",
    "    x = label.transpose().flatten()\n",
    "    y = np.where(x > 0)[0]\n",
    "    if len(y) < 10:  # consider as empty\n",
    "        return ''\n",
    "    z = np.where(np.diff(y) > 1)[0]\n",
    "    start = np.insert(y[z+1], 0, y[0])\n",
    "    end = np.append(y[z], y[-1])\n",
    "    length = end - start\n",
    "    res = [[s+1, l+1] for s, l in zip(list(start), list(length))]\n",
    "    res = list(chain.from_iterable(res))\n",
    "    return ' '.join([str(r) for r in res])\n",
    "\n",
    "\n",
    "def submission():\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = np.load('imgs_mask_test.npy')\n",
    "\n",
    "    argsort = np.argsort(imgs_id_test)\n",
    "    imgs_id_test = imgs_id_test[argsort]\n",
    "    imgs_test = imgs_test[argsort]\n",
    "\n",
    "    total = imgs_test.shape[0]\n",
    "    ids = []\n",
    "    rles = []\n",
    "    for i in range(total):\n",
    "        img = imgs_test[i, 0]\n",
    "        img = prep(img)\n",
    "        rle = run_length_enc(img)\n",
    "\n",
    "        rles.append(rle)\n",
    "        ids.append(imgs_id_test[i])\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print('{}/{}'.format(i, total))\n",
    "\n",
    "    first_row = 'img,pixels'\n",
    "    file_name = 'submission6.csv'\n",
    "\n",
    "    with open(file_name, 'w+') as f:\n",
    "        f.write(first_row + '\\n')\n",
    "        for i in range(total):\n",
    "            s = str(ids[i]) + ',' + rles[i]\n",
    "            f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_test, imgs_id = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Epoch 1/20\n",
      "10842/10842 [==============================] - 467s - loss: -0.1987 - dice_coef: 0.1987   \n",
      "Epoch 2/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.3909 - dice_coef: 0.3909   \n",
      "Epoch 3/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.4822 - dice_coef: 0.4822   \n",
      "Epoch 4/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.5185 - dice_coef: 0.5185   \n",
      "Epoch 5/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.5427 - dice_coef: 0.5427   \n",
      "Epoch 6/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.5644 - dice_coef: 0.5644   \n",
      "Epoch 7/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.5794 - dice_coef: 0.5794   \n",
      "Epoch 8/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.5917 - dice_coef: 0.5917   \n",
      "Epoch 9/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6028 - dice_coef: 0.6028   \n",
      "Epoch 10/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6153 - dice_coef: 0.6153   \n",
      "Epoch 11/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6265 - dice_coef: 0.6265   \n",
      "Epoch 12/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6344 - dice_coef: 0.6344   \n",
      "Epoch 13/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6466 - dice_coef: 0.6466   \n",
      "Epoch 14/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6533 - dice_coef: 0.6533   \n",
      "Epoch 15/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6649 - dice_coef: 0.6649   \n",
      "Epoch 16/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6734 - dice_coef: 0.6734   \n",
      "Epoch 17/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6823 - dice_coef: 0.6823   \n",
      "Epoch 18/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6886 - dice_coef: 0.6886   \n",
      "Epoch 19/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.6984 - dice_coef: 0.6984   \n",
      "Epoch 20/20\n",
      "10842/10842 [==============================] - 461s - loss: -0.7069 - dice_coef: 0.7069   \n",
      "------------------------------\n",
      "Loading and preprocessing test data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Loading saved weights...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Predicting masks on test data...\n",
      "------------------------------\n",
      "5508/5508 [==============================] - 72s    \n",
      "------------------------------\n",
      "finish...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5508\n",
      "1000/5508\n",
      "2000/5508\n",
      "3000/5508\n",
      "4000/5508\n",
      "5000/5508\n"
     ]
    }
   ],
   "source": [
    "submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### result 0.62574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
